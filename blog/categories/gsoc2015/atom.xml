<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Gsoc2015 | Pony's Stable]]></title>
  <link href="http://chienlima.github.io/blog/categories/gsoc2015/atom.xml" rel="self"/>
  <link href="http://chienlima.github.io/"/>
  <updated>2015-07-19T15:01:43+08:00</updated>
  <id>http://chienlima.github.io/</id>
  <author>
    <name><![CDATA[Chienli Ma]]></name>
    <email><![CDATA[maqianlie@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Putting Hand on OpFromGraph]]></title>
    <link href="http://chienlima.github.io/blog/2015/07/18/put-hand-on-opfromgraph/"/>
    <updated>2015-07-18T18:01:35+08:00</updated>
    <id>http://chienlima.github.io/blog/2015/07/18/put-hand-on-opfromgraph</id>
    <content type="html"><![CDATA[<p>This two week, I start working on <code>OpFromGraph</code>. Which is the second part of the proposal.</p>

<p>Currently, if a FunctionGraph have repeated subgraph, theano will optimize these sub-graphs individually, which is not only a waste of computational resources but a waste of time. If we can extract a common structure in FunctionGraph and make it a <code>Op</code>, we can only optimize the sub-graph of this <code>Op</code> once and reuse it every where. This will speed up the optimization process. And <code>OpFromGraph</code> provides such feature.</p>

<p>To make <code>OpFromGraph</code> works well, it should support GPU and can be optimized. Following feature are expected:</p>

<ul>
<li><code>__eq__()</code> and <code>__hash__()</code></li>
<li><code>connection_pattern()</code> and &#8220;infer__shape()&#8220;`</li>
<li>Support GPU</li>
<li><code>c_code()</code></li>
</ul>


<p>I implement two feature in last two week: <code>connection_pattern</code> and <code>infer_shape</code>.  I hope I can make  <code>OpFromGraph</code> a useful feature at the end of this GSoC :).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evaluation Passed and the Next Step: OpFromGraph]]></title>
    <link href="http://chienlima.github.io/blog/2015/07/04/newpost/"/>
    <updated>2015-07-04T23:51:57+08:00</updated>
    <id>http://chienlima.github.io/blog/2015/07/04/newpost</id>
    <content type="html"><![CDATA[<p>Evaluation passed and the next step: OpFromGraph</p>

<p>The PR of <code>function.copy()</code> is ready to merged, only need fred to fix a small bug. And in this Friday I passed the mid-term evaluation. So it&rsquo;s time to take the next step.</p>

<p>In the original proposal ,the next step is to <code>swap output and updates</code>. After a discussion with Fred, we thought this feature is useless so we skip this and head to the next feature directly &ndash; <code>OpFromGraph</code>.</p>

<h2>Goal: </h2>

<p>make class <code>OpFromGraph</code> work.</p>

<h2>Big How?</h2>

<p> <code>OpFromGraph</code> should init a <code>gof.op</code> that has no difference with other <code>Op</code>s and can be optimized. Otherwise it has no sense.</p>

<p>For this, we need to make it work on GPU, make sure it works with C code and document it. Make sure <code>infer_shape()</code>, <code>grad()</code> work with it. Ideally, make <code>R_op()</code> work too.</p>

<h2>Detailed how.</h2>

<ul>
<li>Implement <code>__hash__()</code> and <code>__eq__()</code> method so it is a basic</li>
<li>Implement <code>infer_shape()</code> method so that it&rsquo;s optimizable</li>
<li>test if it work with shared variable as input and if not make it work. Add test for that.</li>
<li>Move it correctly to the GPU. We can do it quickly for the old back-end, move all float32 inputs to the GPU. Otherwise, we need to compile the inner function, see which inputs get moved to the GPU, then create a new OpFromGraph with the corresponding input to the GPU. <a href="https://github.com/Theano/Theano/pull/2982">#2982</a></li>
<li>Maker<code>grad()</code> work. This should remove the grad_depth parameter</li>
</ul>


<hr />

<h3>First Step: infer_shape:</h3>

<p>The main idea is to calculatet the shapes of outputs from given input shapes. This is a process similar to executing a function &ndash; we cannot know the shape of a variable before knowing the shape of the variables it depends on. So, we can mimic the <code>make_thunk()</code> method to calculate the shape from output to input. I come out with a draft now, and need some help with test case.</p>

<pre><code class="python">order = self.fn.maker.fgraph.toposort()

# A dict that map variable to its shape(list)
shape_map = {}

# set the input shape of the fgraph
for in_var, shape in izip(in_vars, shapes);
    shape_map.set_default(in_var, shape)

# calculate the output shape from input shape
for node in order:
    assert all([var in shape_map.keys() for var in node.inputs])

    # calculate output shape
    in_shapes = [ shape_map[var] for var in node.inputs]
    out_shapes = node.op.infer_shape(node, in_shapes)

    # store the shape of that variable
    for out_var, shape in izip(node.outputs, out_shapes):
        shape_map.set_default(out_var, list(shape))

# extract output shape
return [ tuple(shape_map[var]) for var in fgraph.outputs]
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Second Two-week]]></title>
    <link href="http://chienlima.github.io/blog/2015/06/21/the-second-two-week/"/>
    <updated>2015-06-21T00:09:54+08:00</updated>
    <id>http://chienlima.github.io/blog/2015/06/21/the-second-two-week</id>
    <content type="html"><![CDATA[<p>Almost forget to update a post.</p>

<p>In this two week, I finished the first feature &ldquo;Allow user to regenerate a function from compiled one&rdquo;, and this feature &ldquo;can be merged. But there&rsquo;s another PR need to rebase.&rdquo;  So, it&rsquo;s done.</p>

<p>Also, I get a draft of the code that allow user to swap SharedVariable. When I said &lsquo;draft&rsquo;, I mean that I&rsquo;ve finish the code as well as the testcase and they work. I&rsquo;ll make a PR for review at the beginning of next week. Also I have some new idea need to discuss with Fred.</p>

<p>I hope I can finish all 3 feature in the first 6-week: copy, swap_sharedvariable and delete_update. So that I can focus on OpFromGraph in the next half. It seems that someone has started working on it now. I hope he did not &lsquo;rob&rsquo; my job. :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Second_look_into_Theano_core]]></title>
    <link href="http://chienlima.github.io/blog/2015/05/17/second-look-into-theano-core/"/>
    <updated>2015-05-17T19:26:55+08:00</updated>
    <id>http://chienlima.github.io/blog/2015/05/17/second-look-into-theano-core</id>
    <content type="html"><![CDATA[<p>The first feature was done and is being reviewed! This post is my reading notes of theano core this week and how I implements the share_memory feature.</p>

<h2>Data sturctures</h2>

<h3>Function</h3>

<pre><code>Function is a callable object whose core is a function ```fn``` generate by linker. Every time a function is call, it will first examinate input data and then evaluate the ```fn``` to get output value.
</code></pre>

<p>PARAMETERS:</p>

<ul>
<li><strong>input/output_storages:</strong> two list of Container instance.</li>
<li><strong>maker:</strong> the maker of this function</li>
<li><strong>finder/inv_finder:</strong> provide mapping between <code>Container</code> and <code>In</code>( seems useless )</li>
<li><strong>fn:</strong> Core, what the real &lsquo;function&rsquo; is, a python function that evaluate graph.</li>
<li><strong>default and indices:</strong> List of tuples, partially useful. <code>default[i][2]</code> is <code>input_storage[i]</code> ;  <code>indices[i][0]</code> is <code>inputs[i]</code> in FunctionMaker.</li>
</ul>


<p>METHODS:</p>

<ul>
<li><code>__init__()</code>: Initialize input containers value and set up &ldquo;[]&rdquo; operator of container and self</li>
<li><code>__call__()</code>: verify input data types and then execute the <code>self.fn</code>. Pickup value in some of the the output storages and set up corresponding input_storage in there&rsquo;s updates.</li>
</ul>


<hr />

<h3>FunctionMaker</h3>

<pre><code>FunctionMaker is a Factory that create function. However, it's not like a factory very much cause every maker only corrsponds to one function. In FunctionMaker some important info of a theano.function are stored, such as Inputs/Outputs(represented by SymbolicKit), FunctionGraph.
</code></pre>

<p>PARAMS:</p>

<ul>
<li><strong>indices:</strong> ( almost deprecated, ignore )</li>
<li><strong>inputs:</strong> List of In() instances. In()</li>
<li><strong>output:</strong> List of Out() instances</li>
<li><strong>expanded_inputs:</strong> equals to inputs plus sharedvariables</li>
<li><strong>fgraph:</strong> FunctionGraph that represents the function it creates.</li>
<li><strong>Linker:</strong> Linker that link storage and apply method to create a <code>fn</code>. By default, <code>FAST_RUN</code> mode use <code>VM_Linker</code> , <code>FAST_COMPILE</code> uses <code>PerformLinker</code>.</li>
<li><strong>Optimizer, mode, profile &hellip;</strong>: some configuration that has less to do with my job</li>
</ul>


<p>METHOD:</p>

<ul>
<li><strong>create(input_storages):</strong> Given input_storages(by default, list of In.value), start the linker and link the function using <code>Linker.make_thunk()</code> return a theano.function.</li>
</ul>


<hr />

<h3>Linker/VM</h3>

<pre><code>Linker is a class that allocate storage for allpy_nodes and link them together. Understanding Linker and FunctionGraph definitely helps understands how theano works. The core method of Linker is make_thunk(). 
</code></pre>

<p>PARAMS:</p>

<ul>
<li><strong>fgraph</strong>: <code>FunctionGraph</code> accpeted by <code>Linker.accept()</code>.</li>
<li><strong>allow_gc,recycle&hellip;</strong>: some configuration bools</li>
</ul>


<p>METHODS:</p>

<ul>
<li><strong>make_thunk/all&hellip; :</strong>  <code>make_thunk()</code> is defined in class <code>Linker</code>. It calls method <code>make_all()</code>. Every subclass of linker will have slightly different implementation of <code>make_all()</code>. Bascially, at first, <code>make_all()</code> will toposort a fgraph, to acquire the <code>order</code> that apply_nodes should be executed. Next, it will call <code>Op.make_thunk()</code>, whick return a function. This function take input_storage of node, apply computation on them, and put result on output storage. Meanwhile, <code>make_all()</code> will allocate storage for all variables . Same variables in different node will have same storages. This is done by a dict <code>sotarge_map</code> that map variable to storage. At last, Linker return a function that executes list of thunks in certain order to acquire function outputs data.</li>
</ul>


<h3>Storage</h3>

<pre><code>Storage is quite a tricky thing in theano. Theano use a list with one element to be a storage. The element is true data. But all object only refer to the list. 
The list works like a pointer. When some objects refer to a storage, they refers to the list, not the true data. Therefore, modifying the data of storage will not change this reference. By doing this, theano can access and modify storage data from several places without change the reference relationship.
</code></pre>

<hr />

<h3>FunctionGraph</h3>

<pre><code>A representation of the computational graph of a function. It stores given the input and output variables of one function, by calling node.input and variable.owner recursively we can get the whole graph 
</code></pre>

<p>PARAMS:</p>

<ul>
<li><strong>features:</strong> Still Don&rsquo;t understand now, ignore it.</li>
<li><strong>input/output:</strong> List of input/output variabls.</li>
<li><strong>variables:</strong> Set of variables( all variables in the subgraph)</li>
<li><strong>apply_nodes:</strong> Set of apply_nodes in the subgraph defined by inputs and outputs</li>
</ul>


<p>METHODS</p>

<ul>
<li><code>__import_r__</code> and <code>__import__()</code>: import variable and apply_node to this fgraph.</li>
<li><code>clone_get_equiv</code> : Clone fgraph. Return new fgraph and a dict that map old variables to new variables.</li>
<li><code>replace()</code> : Replace all certain variables in fgraph by given new variables.</li>
</ul>


<hr />

<h2>How theano work?( Simplified version )</h2>

<h3>Without update:</h3>

<p>1.Input Variables will be wraped and become <code>In()</code>s. Each <code>In()</code> contains variable, it&rsquo;s value( defaults is none ) as well as some other configuration.
2.Generate fgraph by input and output variables, and then optimize it.
3.Linker toposorts fgraph to get an <code>order</code> of apply_nodes.Next, Linker allocates storages and links function based on this <code>order</code>.
4.Done</p>

<h3>With update:</h3>

<p>Update is given by a dict <code>{ ori_var:update_var ... }</code>. <code>Ori_var</code> is limited to be an SharedVariable therefore it will transfer into <code>In()</code> and become the input of this function. <code>update_var</code> will be add into the output of fgraph. Everytime a function called, the function  extract the storage of <code>update_var</code> and use it to set the value of corresponding input.</p>

<hr />

<h2>How to implement the share_memory feature?</h2>

<p>This is simple after understand how theano works. I implements it following sevaral steps:
1. Copy <code>In()</code> and <code>Out()</code> instances in Maker
2. Copy fgraph and get the dict <code>equiv</code> that map old variables to new variables
3. Copy old storage_map in <code>Function.fn.storage_map</code>
4. Modify copied storage_map accord to equiv, so that in the copied storage_map, new variables are mapped to old storage if memory need to be shared.
5. Reinitialize the maker, linke the function using the copied storage_map
6. Done</p>

<hr />

<p>Ok, basically this is the report of this week&rsquo;s work. Now I need to figure out how to implement the following features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First Gift From Google]]></title>
    <link href="http://chienlima.github.io/blog/2015/05/16/first-gift-from-google/"/>
    <updated>2015-05-16T16:12:52+08:00</updated>
    <id>http://chienlima.github.io/blog/2015/05/16/first-gift-from-google</id>
    <content type="html"><![CDATA[<p>Several days a ago, I reveived a Fedex package. When I opened it &hellip; Bomb!</p>

<h4>Payoneer pre-paid MasterCard with Google icon on it!</h4>

<p><img src="/images/GSoCMasterCard.jpg" width="512" height="512" title="&lsquo;image&rsquo; &lsquo;images&rsquo;" ></p>

<p>Good looking card. I guass someone will regret for using existing bank account.
Also this is my first own credit card. Finally I can make payment without a phone from my dad :)</p>

<p>Google is ready to pay me now. &ldquo;Are you OK?&rdquo;( Leijun&rsquo;s &lsquo;Chinglish&rsquo; )</p>

<hr />

<p>Kinda occupied in the last week. Luckily, I finished those buinesses. And now I am back again.
The first feature I will add to theano is a function that allow user to make a copy of function and allow functions run multi-theadedly. There exist two similar features: <code>pickle()</code> and <code>copy()</code> of a function. To figure how I need to work. I need to take 3 steps as preparation:</p>

<ul>
<li>Look into the code and see how function is generated <code>Done</code></li>
<li>Look into the code and understand <code>Function</code>, <code>FunctionMaker</code>, <code>Linker</code> and <code>FunctionGraph</code>. <code>next</code></li>
<li>Have a look at <code>pickle</code> and <code>copy()</code>, use them, figure out how them work and the differneces. Then think about my own idea.</li>
</ul>


<p>Ok, now I need to go and take the step two now.</p>
]]></content>
  </entry>
  
</feed>
